<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AutoMR: A Universal Time Series Motion Recognition Pipeline</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              Likun Zhang<sup>1,2</sup> <a href="mailto:likun.zhang@openinterx.com">&lt;likun.zhang@openinterx.com&gt;</a>
            </span>,
            <span class="author-block">
              Sicheng Yang<sup>1</sup> <a href="mailto:940863869@qq.com">&lt;940863869@qq.com&gt;</a>
            </span>,
            <span class="author-block">
              Zhuo Wang<sup>1</sup> <a href="mailto:zhuohci01@gmail.com">&lt;zhuohci01@gmail.com&gt;</a>
            </span>,
            <span class="author-block">
              Haining Liang<sup>3</sup> <a href="mailto:hainingliang@hkust-gz.edu.cn">&lt;hainingliang@hkust-gz.edu.cn&gt;</a>
            </span>,
            <span class="author-block">
              Junxiao Shen<sup>4 <span style="font-weight: bold;">*</span></sup> <a href="mailto:junxiao.shen@bristol.ac.uk">&lt;junxiao.shen@bristol.ac.uk&gt;</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">1 OpenInterX</span>,
            <span class="author-block">2 University of California, Berkeley</span>,
            <span class="author-block">3 HKUST (Guangzhou)</span>,
            <span class="author-block">4 University of Bristol</span>
          </div>

          <!-- Corresponding Author footnote -->
          <div class="is-size-5 publication-authors">
            <span class="eql-cntrb"><small><br><sup>*</sup> Corresponding Author</small></span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/liquora/AutoMR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- 
  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
<!--         <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>  -->
  -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we present an end-to-end automated motion recognition (AutoMR) pipeline designed for multimodal datasets. The proposed framework seamlessly integrates data preprocessing, model training, hyperparameter tuning, and evaluation, enabling robust performance across diverse scenarios. Our approach addresses two primary challenges: 1) variability in sensor data formats and parameters across datasets, which traditionally requires task-specific machine learning implementations, and 2) the complexity and time consumption of hyperparameter tuning for optimal model performance. Our library features an all-in-one solution incorporating QuartzNet as the core model, automated hyperparameter tuning, and comprehensive metrics tracking. Extensive experiments demonstrate its effectiveness on 10 diverse datasets, achieving state-of-the-art performance. This work lays a solid foundation for deploying motion-capture solutions across varied real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/Challenge.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Key challenges in motion recognition: (1) Dataset variability across different sensor types increases preprocessing complexity, (2) Model adaptation requires dataset-specific optimizations, leading to high computational costs, (3) Hyperparameter tuning demands expertise and computational resources, and (4) Deployment barriers limit accessibility for non-specialists. These challenges highlight the need for a scalable and automated framework.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/process.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          AutoMR end-to-end architecture, illustrating the complete workflow from data preprocessing to model training and hyperparameter tuning. The framework standardizes diverse datasets, selects optimal model configurations, and ensures efficient training and deployment for motion recognition across different sensor modalities.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/dependcies.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         AutoMR's modular and hierarchical architecture, illustrating the interaction between core modules and dataset-specific training scripts. The upper layer consists of fundamental components for dataset preprocessing, augmentation, model selection, training, and hyperparameter tuning, ensuring a standardized and optimized workflow. The lower layer contains dataset-specific execution scripts that utilize these core modules for model training on individual datasets, enabling efficient adaptation across diverse sensor modalities.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/overall_accuracy.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Overall accuracy comparison between AutoMR and SOTA models across ten datasets. AutoMR achieves superior performance on eight datasets, highlighting its effectiveness in generalizing across diverse gesture recognition tasks.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/ablation.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Ablation study comparing automatic and manual hyperparameter tuning across four key metrics: accuracy, precision, recall, and F1-score. The results show that AutoMR’s automatic tuning achieves performance comparable to manual tuning, demonstrating its reliability for real-world applications.
      </h2>
    </div>
        
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
<!--       <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
<!--             <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
